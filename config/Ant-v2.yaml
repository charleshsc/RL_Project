env_name: Ant-v2
min_action_value: -1
max_action_value: 1
action_dimention: 8
state_dimention: 111
seed: 0

A3C:
  env_name: Ant-v2
  min_action_value: -1
  max_action_value: 1
  action_dimention: 8
  state_dimention: 111
  max_epochs: 1000
  max_epoch_steps: 1000
  update_global_iter: 10
  lr: 0.0001
  epoch_to_save: 50
  gamma: 0.9

TD3:
  env_name: Ant-v2
  min_action_value: -1
  max_action_value: 1
  action_dimention: 8
  state_dimention: 111
  capacity: 1000000
  batch_size: 256
  seed: 0
  start_timesteps: 25000  # Time steps initial random policy is used
  eval_freq: 5000         # How often (time steps) we evaluate
  max_timesteps: 1000000  # Max time steps to run environment
  expl_noise: 0.1         # Std of Gaussian exploration noise
  discount: 0.99          # Discount factor
  tau: 0.005              # Target network update rate
  policy_noise: 0.2       # Noise added to target policy during critic update
  noise_clip: 0.5         # Range to clip target policy noise
  policy_freq: 2          # Frequency of delayed policy updates

SAC:
  env_name: Ant-v2
  min_action_value: -1
  max_action_value: 1
  action_dimention: 8
  state_dimention: 111
  capacity: 1000000
  batch_size: 256
  seed: 0
  start_timesteps: 25000  # Time steps initial random policy is used
  eval_freq: 5000         # How often (time steps) we evaluate
  max_timesteps: 1000000  # Max time steps to run environment
  lr: 0.0003
  discount: 0.99
  tau: 0.005
  log_std_max: 2
  log_std_min: -20
  alpha: 0.2
  automatic_entropy_tuning: 0

DDPG:
  env_name: Ant-v2
  min_action_value: -1
  max_action_value: 1
  action_dimention: 8
  state_dimention: 111
  capacity: 1000000
  batch_size: 256
  seed: 0
  start_timesteps: 25000  # Time steps initial random policy is used
  eval_freq: 5000         # How often (time steps) we evaluate
  max_timesteps: 1000000  # Max time steps to run environment
  discount: 0.99          # Discount factor
  tau: 0.005              # Target network update rate
  lr: 0.001


